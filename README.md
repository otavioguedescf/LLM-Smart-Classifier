# üß† LLM Smart Classifier

Projeto completo de classifica√ß√£o inteligente de tweets humanit√°rios, utilizando Big Data com PySpark, Machine Learning supervisionado, Modelos de Linguagem (LLMs), MLOps e deploy em nuvem.

---

## üöÄ Vis√£o Geral

Este projeto tem como objetivo construir uma solu√ß√£o capaz de **entender, categorizar e explicar automaticamente milhares de mensagens humanit√°rias** postadas em redes sociais durante crises e emerg√™ncias.

Por meio de uma abordagem moderna e escal√°vel, unimos t√©cnicas de NLP com modelos cl√°ssicos de machine learning, modelos de linguagem (LLMs), pipelines de MLOps e interfaces de uso para entregar um sistema robusto e aplic√°vel em cen√°rios reais.

---

## üß© Estrutura Modular do Projeto

| M√≥dulo | Descri√ß√£o |
|--------|-----------|
| [1. Ingest√£o e Pr√©-processamento](#modulo-1--ingestao-e-pre-processamento-com-pyspark) | Leitura distribu√≠da dos dados com PySpark, limpeza e tokeniza√ß√£o |
| [2. An√°lise Explorat√≥ria de Dados](#modulo-2--analise-exploratoria-de-dados-eda) | Explora√ß√£o, frequ√™ncia de palavras, n-gramas e padr√µes |
| [3. Engenharia de Features e Modelagem](#modulo-3--engenharia-de-features-e-modelagem) | Vetoriza√ß√£o, modelos supervisionados e tracking com MLFlow |
| [4. Aplica√ß√£o de LLMs](#modulo-4--aplicacao-de-llms) | Embeddings, zero-shot, few-shot, explica√ß√µes e compara√ß√£o |
| [5. MLOps e Deploy](#modulo-5--mlops-e-deploy) | Pipeline, API, Docker, Azure ML, CI/CD |
| [6. Valida√ß√£o Avan√ßada](#modulo-6--validacao-avancada-e-ab-testing) | Testes estat√≠sticos e compara√ß√£o entre abordagens |
| [7. Interface Visual Interativa](#modulo-7--interface-visual-interativa) | App com input e explica√ß√µes acess√≠veis |
| [8. Documenta√ß√£o e Storytelling T√©cnico](#modulo-8--documentacao-e-storytelling-tecnico) | README, prints, impacto e vis√£o estrat√©gica |

---

## üîπ M√≥dulo 1 ‚Äì Ingest√£o e Pr√©-processamento com PySpark

**Objetivo:**  
Ingerir os dados brutos no ambiente distribu√≠do do PySpark, realizar o pr√©-processamento textual necess√°rio para an√°lise posterior e salvar os dados limpos em formato otimizado (Parquet).

---

### 1.1 Leitura do dataset no ambiente distribu√≠do

- O dataset foi carregado diretamente no ambiente Databricks com PySpark, contendo aproximadamente **47.868 tweets** humanit√°rios.
- Cada linha representa uma mensagem classificada por categoria humanit√°ria (como sa√∫de, abrigo, comida, etc.).

*Trecho de c√≥digo:*
```python
# Copiando o ZIP da DBFS para o sistema de arquivos local do cluster
dbutils.fs.cp("dbfs:/FileStore/tables/humaid_eventwise.zip", "file:/tmp/humaid_eventwise.zip")

# Extraindo
with zipfile.ZipFile("/tmp/humaid_eventwise.zip", 'r') as zip_ref:
    zip_ref.extractall("/tmp/humaid_eventwise")
