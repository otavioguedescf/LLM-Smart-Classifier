# üß† LLM Smart Classifier

Projeto completo de classifica√ß√£o inteligente de tweets humanit√°rios, utilizando Big Data com PySpark, Machine Learning supervisionado, Modelos de Linguagem (LLMs), MLOps e deploy em nuvem.

---

## üöÄ Vis√£o Geral

Este projeto tem como objetivo construir uma solu√ß√£o capaz de **entender, categorizar e explicar automaticamente milhares de mensagens humanit√°rias** postadas em redes sociais durante crises e emerg√™ncias.

Por meio de uma abordagem moderna e escal√°vel, unimos t√©cnicas de NLP com modelos cl√°ssicos de machine learning, modelos de linguagem (LLMs), pipelines de MLOps e interfaces de uso para entregar um sistema robusto e aplic√°vel em cen√°rios reais.

---

## üß© Estrutura Modular do Projeto

| M√≥dulo | Descri√ß√£o |
|--------|-----------|
| [1. Ingest√£o e Pr√©-processamento](#modulo-1--ingestao-e-pre-processamento-com-pyspark) | Leitura distribu√≠da dos dados com PySpark, limpeza e tokeniza√ß√£o |
| [2. An√°lise Explorat√≥ria de Dados](#modulo-2--analise-exploratoria-de-dados-eda) | Explora√ß√£o, frequ√™ncia de palavras, n-gramas e padr√µes |
| [3. Engenharia de Features e Modelagem](#modulo-3--engenharia-de-features-e-modelagem) | Vetoriza√ß√£o, modelos supervisionados e tracking com MLFlow |
| [4. Aplica√ß√£o de LLMs](#modulo-4--aplicacao-de-llms) | Embeddings, zero-shot, few-shot, explica√ß√µes e compara√ß√£o |
| [5. MLOps e Deploy](#modulo-5--mlops-e-deploy) | Pipeline, API, Docker, Azure ML, CI/CD |
| [6. Valida√ß√£o Avan√ßada](#modulo-6--validacao-avancada-e-ab-testing) | Testes estat√≠sticos e compara√ß√£o entre abordagens |
| [7. Interface Visual Interativa](#modulo-7--interface-visual-interativa) | App com input e explica√ß√µes acess√≠veis |
| [8. Documenta√ß√£o e Storytelling T√©cnico](#modulo-8--documentacao-e-storytelling-tecnico) | README, prints, impacto e vis√£o estrat√©gica |

---

## üîπ M√≥dulo 1 ‚Äì Ingest√£o e Pr√©-processamento com PySpark

**Objetivo:**  
Ingerir os dados brutos no ambiente distribu√≠do do PySpark, realizar o pr√©-processamento textual necess√°rio para an√°lise posterior e salvar os dados limpos em formato otimizado (Parquet).

---

### 1.1 Leitura do dataset no ambiente distribu√≠do

- O dataset foi carregado diretamente no ambiente Databricks com PySpark, contendo aproximadamente **47.868 tweets** humanit√°rios.
- Cada linha representa uma mensagem classificada por categoria humanit√°ria (como sa√∫de, abrigo, comida, etc.).

*Trecho de c√≥digo:*
```python
# Copiando o ZIP da DBFS para o sistema de arquivos local do cluster
dbutils.fs.cp("dbfs:/FileStore/tables/humaid_eventwise.zip", "file:/tmp/humaid_eventwise.zip")

# Extraindo
with zipfile.ZipFile("/tmp/humaid_eventwise.zip", 'r') as zip_ref:
    zip_ref.extractall("/tmp/humaid_eventwise")

---

### 1.2 Limpeza textual e normaliza√ß√£o

**Objetivo:**  
Preparar os textos dos tweets para processamento lingu√≠stico, removendo ru√≠dos e padronizando a estrutura textual. Esta etapa √© essencial para melhorar a qualidade dos dados e garantir resultados mais precisos nos pr√≥ximos m√≥dulos de an√°lise e modelagem.

---

**Transforma√ß√µes aplicadas:**

- ‚úÖ Convers√£o de todos os caracteres para min√∫sculas;
- ‚úÖ Remo√ß√£o de:
  - **Links/URLs** (ex: `http://...`);
  - **Men√ß√µes** (ex: `@username`);
  - **Hashtags** (ex: `#emerg√™ncia`);
  - **Pontua√ß√£o e caracteres especiais** (ex: `!`, `?`, `...`);
  - **Emojis** e **s√≠mbolos n√£o textuais**;
  - **M√∫ltiplos espa√ßos em branco**.

---

**Nova coluna criada:** `text_clean`  
Essa coluna representa o conte√∫do textual limpo de cada tweet.

---

*Trecho de c√≥digo:*
```python
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import re

# Fun√ß√£o Python para limpeza
def limpar_texto(texto):
    if texto:
        texto = texto.lower()
        texto = re.sub(r"http\S+", "", texto)                     # Remove links
        texto = re.sub(r"rt\s+@[\w_]+:? ?", "", texto)            # Remove RTs com @usuario
        texto = re.sub(r"@\w+", "", texto)                        # Remove todas as outras men√ß√µes
        texto = re.sub(r"#\w+", "", texto)                        # Remove hashtags
        texto = re.sub(r"[^\w\s]", "", texto)                     # Remove pontua√ß√£o
        texto = re.sub(r"\s+", " ", texto).strip()                # Espa√ßos extras
    return texto

# Registrando a fun√ß√£o como UDF do PySpark
limpar_udf = udf(limpar_texto, StringType())

# Criando nova coluna com texto limpo
df_limpo = df_total.withColumn("text_clean", limpar_udf(col("tweet_text")))

# Visualizando resultado
df_limpo.select("tweet_text", "text_clean").show(10, truncate=False)
