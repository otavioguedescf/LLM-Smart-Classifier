# üß† LLM Smart Classifier

Projeto completo de classifica√ß√£o inteligente de tweets humanit√°rios, utilizando Big Data com PySpark, Machine Learning supervisionado, Modelos de Linguagem (LLMs), MLOps e deploy em nuvem.

---

## üöÄ Vis√£o Geral

Este projeto tem como objetivo construir uma solu√ß√£o capaz de **entender, categorizar e explicar automaticamente milhares de mensagens humanit√°rias** postadas em redes sociais durante crises e emerg√™ncias.

Por meio de uma abordagem moderna e escal√°vel, unimos t√©cnicas de NLP com modelos cl√°ssicos de machine learning, modelos de linguagem (LLMs), pipelines de MLOps e interfaces de uso para entregar um sistema robusto e aplic√°vel em cen√°rios reais.

---

## üß© Estrutura Modular do Projeto

| M√≥dulo | Descri√ß√£o |
|--------|-----------|
| [1. Ingest√£o e Pr√©-processamento](#modulo-1--ingestao-e-pre-processamento-com-pyspark) | Leitura distribu√≠da dos dados com PySpark, limpeza e tokeniza√ß√£o |
| [2. An√°lise Explorat√≥ria de Dados](#modulo-2--analise-exploratoria-de-dados-eda) | Explora√ß√£o, frequ√™ncia de palavras, n-gramas e padr√µes |
| [3. Engenharia de Features e Modelagem](#modulo-3--engenharia-de-features-e-modelagem) | Vetoriza√ß√£o, modelos supervisionados e tracking com MLFlow |
| [4. Aplica√ß√£o de LLMs](#modulo-4--aplicacao-de-llms) | Embeddings, zero-shot, few-shot, explica√ß√µes e compara√ß√£o |
| [5. MLOps e Deploy](#modulo-5--mlops-e-deploy) | Pipeline, API, Docker, Azure ML, CI/CD |
| [6. Valida√ß√£o Avan√ßada](#modulo-6--validacao-avancada-e-ab-testing) | Testes estat√≠sticos e compara√ß√£o entre abordagens |
| [7. Interface Visual Interativa](#modulo-7--interface-visual-interativa) | App com input e explica√ß√µes acess√≠veis |
| [8. Documenta√ß√£o e Storytelling T√©cnico](#modulo-8--documentacao-e-storytelling-tecnico) | README, prints, impacto e vis√£o estrat√©gica |

---

## üîπ M√≥dulo 1 ‚Äì Ingest√£o e Pr√©-processamento com PySpark

**Objetivo:**  
Ingerir os dados brutos no ambiente distribu√≠do do PySpark, realizar o pr√©-processamento textual necess√°rio para an√°lise posterior e salvar os dados limpos em formato otimizado (Parquet).

---

### 1.1 Leitura do dataset no ambiente distribu√≠do

- O dataset foi carregado diretamente no ambiente Databricks com PySpark, contendo aproximadamente **47.868 tweets** humanit√°rios.
- Cada linha representa uma mensagem classificada por categoria humanit√°ria (como sa√∫de, abrigo, comida, etc.).

*Trecho de c√≥digo:*
```python
# Copiando o ZIP da DBFS para o sistema de arquivos local do cluster
dbutils.fs.cp("dbfs:/FileStore/tables/humaid_eventwise.zip", "file:/tmp/humaid_eventwise.zip")

# Extraindo
with zipfile.ZipFile("/tmp/humaid_eventwise.zip", 'r') as zip_ref:
    zip_ref.extractall("/tmp/humaid_eventwise")
```

---

### 1.2 Limpeza textual e normaliza√ß√£o

**Objetivo:**  
Preparar os textos dos tweets para processamento lingu√≠stico, removendo ru√≠dos e padronizando a estrutura textual. Esta etapa √© essencial para melhorar a qualidade dos dados e garantir resultados mais precisos nos pr√≥ximos m√≥dulos de an√°lise e modelagem.

---

**Transforma√ß√µes aplicadas:**

- ‚úÖ Convers√£o de todos os caracteres para min√∫sculas;
- ‚úÖ Remo√ß√£o de:
  - **Links/URLs** (ex: `http://...`);
  - **Men√ß√µes** (ex: `@username`);
  - **Hashtags** (ex: `#emerg√™ncia`);
  - **Pontua√ß√£o e caracteres especiais** (ex: `!`, `?`, `...`);
  - **Emojis** e **s√≠mbolos n√£o textuais**;
  - **M√∫ltiplos espa√ßos em branco**.

---

**Nova coluna criada:** `text_clean`  
Essa coluna representa o conte√∫do textual limpo de cada tweet.

---

*Trecho de c√≥digo:*
```python
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import re

# Fun√ß√£o Python para limpeza
def limpar_texto(texto):
    if texto:
        texto = texto.lower()
        texto = re.sub(r"http\S+", "", texto)                     # Remove links
        texto = re.sub(r"rt\s+@[\w_]+:? ?", "", texto)            # Remove RTs com @usuario
        texto = re.sub(r"@\w+", "", texto)                        # Remove todas as outras men√ß√µes
        texto = re.sub(r"#\w+", "", texto)                        # Remove hashtags
        texto = re.sub(r"[^\w\s]", "", texto)                     # Remove pontua√ß√£o
        texto = re.sub(r"\s+", " ", texto).strip()                # Espa√ßos extras
    return texto

# Registrando a fun√ß√£o como UDF do PySpark
limpar_udf = udf(limpar_texto, StringType())

# Criando nova coluna com texto limpo
df_limpo = df_total.withColumn("text_clean", limpar_udf(col("tweet_text")))

# Visualizando resultado
df_limpo.select("tweet_text", "text_clean").show(10, truncate=False)
```

---

### 1.3 Tokeniza√ß√£o e remo√ß√£o de stopwords

**Objetivo:**  
Transformar os textos limpos em **listas de palavras relevantes**, removendo palavras comuns (como "the", "and", "is", etc.) que n√£o agregam valor sem√¢ntico. Essa etapa √© essencial para preparar os dados para an√°lise textual e modelagem supervisionada.

---

**Transforma√ß√µes aplicadas:**

- ‚úÖ **Tokeniza√ß√£o:** separa√ß√£o do texto em palavras individuais (*tokens*);
- ‚úÖ **Remo√ß√£o de stopwords:** exclus√£o de palavras muito frequentes e com pouco significado no contexto de NLP;
- ‚úÖ **Nova coluna gerada:** `tokens_filtrados`, contendo as listas de palavras √∫teis.

---

*Trecho de c√≥digo:*
```python
# Etapa 1: Tokeniza√ß√£o
from pyspark.ml.feature import Tokenizer

tokenizer = Tokenizer(inputCol="text_clean", outputCol="tokens")
df_tokens = tokenizer.transform(df_limpo)

# Etapa 2: Remo√ß√£o de stopwords
from pyspark.ml.feature import StopWordsRemover

remover = StopWordsRemover(inputCol="tokens", outputCol="tokens_filtrados")
df_tokens_filtrados = remover.transform(df_tokens)

# Visualizando o resultado
display(df_tokens_filtrados.head(5))
```

---

### 1.4 Salvamento em formato Parquet

**Objetivo:**  
Persistir os dados pr√©-processados em um formato otimizado (`Parquet`), garantindo **efici√™ncia de leitura**, **compacta√ß√£o de armazenamento** e **compatibilidade com processamento distribu√≠do** em etapas futuras do projeto.

---

**Por que Parquet?**

- ‚úÖ Formato **colunar**, ideal para consultas anal√≠ticas;
- ‚úÖ Compat√≠vel com ferramentas distribu√≠das como PySpark;
- ‚úÖ Permite leitura seletiva de colunas, economizando tempo e recursos;
- ‚úÖ Otimiza a performance de etapas como vetoriza√ß√£o e modelagem.

---

*Trecho de c√≥digo:*
```python
# Salvar DataFrame como .parquet
df_tokens_filtrados.write.mode("overwrite").parquet("/tmp/humaid_dados_limpos")

# Leitura futura:
df_pronto= spark.read.parquet("/tmp/humaid_dados_limpos")
df_pronto.show(5, truncate=False)
```

---

## üîπ M√≥dulo 2 ‚Äì An√°lise Explorat√≥ria de Dados (EDA)

**Objetivo:**  
Compreender a estrutura, distribui√ß√£o e principais caracter√≠sticas dos tweets humanit√°rios ap√≥s o pr√©-processamento. Essa etapa permite identificar padr√µes, anomalias e orientar decis√µes futuras na modelagem.

---

### 2.1 Vis√£o Geral dos Dados

**Objetivo da etapa:**  
Realizar uma an√°lise inicial para entender:
- O volume total de registros dispon√≠veis;
- A estrutura e os tipos das colunas;
- Presen√ßa de valores nulos;
- Exemplo de registros ap√≥s limpeza.

---

*Trecho de c√≥digo:*
```python
# Contar o total de linhas
print("Total de tweets:", df_tokens_filtrados.count())

# Ver categorias √∫nicas
df_tokens_filtrados.select("class_label").distinct().show(truncate=False)

# Verificando a contagem de linhas por categoria
df_tokens_filtrados.groupby("class_label").count().orderBy("count", ascending=False).show(truncate=False)
```

---

### 2.2 Distribui√ß√£o das Categorias (Balanceamento)

**Objetivo:**  
Analisar a vari√°vel-alvo `class_label` para entender o **balanceamento entre as categorias humanit√°rias**. Isso √© crucial para identificar poss√≠veis desequil√≠brios que possam influenciar negativamente os modelos de classifica√ß√£o supervisionada.

---

**Transforma√ß√µes e a√ß√µes aplicadas:**

- ‚úÖ Agrupamento por categoria com contagem de ocorr√™ncias;
- ‚úÖ Ordena√ß√£o decrescente para facilitar a an√°lise;
- ‚úÖ Gera√ß√£o de gr√°fico de barras para visualiza√ß√£o do desequil√≠brio.

---

*Trecho de c√≥digo:*
```python
# Agrupar em categorias
df_labels = df_tokens_filtrados.groupBy("class_label").count()

# Ordenar categorias do maior para o menor
df_labels = df_labels.orderBy("count",ascending=False)

# Converter para o pandas
df_labels = df_labels.toPandas()
```

---

### 2.3 Frequ√™ncia de Palavras por Categoria (An√°lise Quantitativa)

**Objetivo:**  
Identificar as **palavras mais frequentes dentro de cada categoria humanit√°ria**, a fim de entender os termos mais caracter√≠sticos de cada tipo de situa√ß√£o. Essa an√°lise ajuda na constru√ß√£o de vetores de texto mais representativos para os modelos.

---

**Transforma√ß√µes e a√ß√µes aplicadas:**

- ‚úÖ Explos√£o da coluna `tokens_filtrados` em palavras individuais;
- ‚úÖ Agrupamento por `class_label` + `token`;
- ‚úÖ Contagem de ocorr√™ncias de cada palavra por categoria;
- ‚úÖ Filtragem das palavras mais frequentes por classe.

---

*Trecho de c√≥digo:*
```python
# Selecionar categorias dominantes
categorias_dominantes = ["rescue_volunteering_or_donation_effort","other_relevant_information","infrastructure_and_utility_damage","sympathy_and_support","injured_or_dead  _people"]

# Filtrar nosso DataFrame com as categorias dominantes
df_filtrado = df_tokens_filtrados.filter(df_tokens_filtrados["class_label"].isin(categorias_dominantes))

df_filtrado.show(5)

# Gerar texto concatenado com categoria
from pyspark.sql.functions import explode, collect_list, concat_ws

# Vamos explodir as palavras
df_explode = df_filtrado.select("class_label", explode("tokens_filtrados").alias("token"))

df_explode.show(5)
```

---

### 2.4 Frequ√™ncia de N-Gramas (Bigramas e Trigramas)

**Objetivo:**  
Explorar **composi√ß√µes de palavras mais comuns** (bigramas e trigramas) nos tweets humanit√°rios. Isso permite capturar estruturas lingu√≠sticas relevantes, como ‚Äúneed food‚Äù, ‚Äúmedical help‚Äù, ou ‚Äúpeople are trapped‚Äù, que carregam mais contexto do que palavras isoladas.

---

**Transforma√ß√µes e a√ß√µes aplicadas:**

- ‚úÖ Utiliza√ß√£o do `NGram` do PySpark para gerar bigramas e trigramas a partir da coluna `tokens_filtrados`;
- ‚úÖ Explos√£o e contagem de n-gramas;
- ‚úÖ An√°lise das express√µes mais frequentes por categoria.

---

*Trecho de c√≥digo (Bigramas):*
```python
from pyspark.sql.functions import size

df_tokens_filtrados = df_tokens_filtrados.withColumn("text_length", size(col("tokens_filtrados")))
df_tokens_filtrados.show(5)

# Estat√≠sticas das palavras
df_tokens_filtrados.select("text_length").describe().show()
```

---

### 2.5 Comprimento dos Textos

**Objetivo:**  
Analisar a **distribui√ß√£o do comprimento dos textos**, medido em n√∫mero de tokens, para entender o n√≠vel de complexidade lingu√≠stica e poss√≠veis padr√µes associados a diferentes categorias humanit√°rias.

---

**Transforma√ß√µes e a√ß√µes aplicadas:**

- ‚úÖ C√°lculo da quantidade de tokens em cada tweet (coluna `tokens_filtrados`);
- ‚úÖ Gera√ß√£o de histogramas para visualizar a distribui√ß√£o geral e por categoria;
- ‚úÖ Identifica√ß√£o de outliers e padr√µes relevantes (ex: categorias com textos mais curtos ou longos).

---

*Trecho de c√≥digo:*
```python
# Explodir palavras por linha
from pyspark.sql.functions import explode

df_exploded_limpo = df_tokens_filtrados_limpos.select("class_label", explode("tokens_filtrados").alias("token"))

# Agrupa por categoria
from pyspark.sql.functions import collect_list

df_grouped_tokens = df_exploded_limpo.groupBy("class_label") \
    .agg(collect_list("token").alias("all_tokens"))

# Gerar unigramas, bigramas e trigramas

import pandas as pd
from nltk.util import ngrams
from collections import Counter

df_tokens_pd = df_grouped_tokens.toPandas()

def extrair_top_ngrams(tokens, n, top_n=10):
    return Counter(ngrams(tokens, n)).most_common(top_n)

rows = []

for _, row in df_tokens_pd.iterrows():
    categoria = row["class_label"]
    tokens = row["all_tokens"]

    # Top palavras, bigramas e trigramas
    top_unigrams = [uni[0] for uni in Counter(tokens).most_common(10)]
    top_bigrams = [" ".join(bi) for bi, _ in extrair_top_ngrams(tokens, 2)]
    top_trigrams = [" ".join(tri) for tri, _ in extrair_top_ngrams(tokens, 3)]

    rows.append({
        "class_label": categoria,
        "top_10_unigrams": ", ".join(top_unigrams),
        "top_10_bigrams": ", ".join(top_bigrams),
        "top_10_trigrams": ", ".join(top_trigrams),
    })

df_ngrams_limpo = pd.DataFrame(rows)
```
---



