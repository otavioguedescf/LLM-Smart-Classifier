# üß† LLM Smart Classifier

Projeto completo de classifica√ß√£o inteligente de tweets humanit√°rios, utilizando Big Data com PySpark, Machine Learning supervisionado, Modelos de Linguagem (LLMs), MLOps e deploy em nuvem.

---

## üöÄ Vis√£o Geral

Este projeto tem como objetivo construir uma solu√ß√£o capaz de **entender, categorizar e explicar automaticamente milhares de mensagens humanit√°rias** postadas em redes sociais durante crises e emerg√™ncias.

Por meio de uma abordagem moderna e escal√°vel, unimos t√©cnicas de NLP com modelos cl√°ssicos de machine learning, modelos de linguagem (LLMs), pipelines de MLOps e interfaces de uso para entregar um sistema robusto e aplic√°vel em cen√°rios reais.

---

## üß© Estrutura Modular do Projeto

| M√≥dulo | Descri√ß√£o |
|--------|-----------|
| [1. Ingest√£o e Pr√©-processamento](#modulo-1--ingestao-e-pre-processamento-com-pyspark) | Leitura distribu√≠da dos dados com PySpark, limpeza e tokeniza√ß√£o |
| [2. An√°lise Explorat√≥ria de Dados](#modulo-2--analise-exploratoria-de-dados-eda) | Explora√ß√£o, frequ√™ncia de palavras, n-gramas e padr√µes |
| [3. Engenharia de Features e Modelagem](#modulo-3--engenharia-de-features-e-modelagem) | Vetoriza√ß√£o, modelos supervisionados e tracking com MLFlow |
| [4. Aplica√ß√£o de LLMs](#modulo-4--aplicacao-de-llms) | Embeddings, zero-shot, few-shot, explica√ß√µes e compara√ß√£o |
| [5. MLOps e Deploy](#modulo-5--mlops-e-deploy) | Pipeline, API, Docker, Azure ML, CI/CD |
| [6. Valida√ß√£o Avan√ßada](#modulo-6--validacao-avancada-e-ab-testing) | Testes estat√≠sticos e compara√ß√£o entre abordagens |
| [7. Interface Visual Interativa](#modulo-7--interface-visual-interativa) | App com input e explica√ß√µes acess√≠veis |
| [8. Documenta√ß√£o e Storytelling T√©cnico](#modulo-8--documentacao-e-storytelling-tecnico) | README, prints, impacto e vis√£o estrat√©gica |

---

## üîπ M√≥dulo 1 ‚Äì Ingest√£o e Pr√©-processamento com PySpark

**Objetivo:**  
Ingerir os dados brutos no ambiente distribu√≠do do PySpark, realizar o pr√©-processamento textual necess√°rio para an√°lise posterior e salvar os dados limpos em formato otimizado (Parquet).

---

### 1.1 Leitura do dataset no ambiente distribu√≠do

- O dataset foi carregado diretamente no ambiente Databricks com PySpark, contendo aproximadamente **47.868 tweets** humanit√°rios.
- Cada linha representa uma mensagem classificada por categoria humanit√°ria (como sa√∫de, abrigo, comida, etc.).

*Trecho de c√≥digo:*
```python
# Copiando o ZIP da DBFS para o sistema de arquivos local do cluster
dbutils.fs.cp("dbfs:/FileStore/tables/humaid_eventwise.zip", "file:/tmp/humaid_eventwise.zip")

# Extraindo
with zipfile.ZipFile("/tmp/humaid_eventwise.zip", 'r') as zip_ref:
    zip_ref.extractall("/tmp/humaid_eventwise")
```

---

### 1.2 Limpeza textual e normaliza√ß√£o

**Objetivo:**  
Preparar os textos dos tweets para processamento lingu√≠stico, removendo ru√≠dos e padronizando a estrutura textual. Esta etapa √© essencial para melhorar a qualidade dos dados e garantir resultados mais precisos nos pr√≥ximos m√≥dulos de an√°lise e modelagem.

---

**Transforma√ß√µes aplicadas:**

- ‚úÖ Convers√£o de todos os caracteres para min√∫sculas;
- ‚úÖ Remo√ß√£o de:
  - **Links/URLs** (ex: `http://...`);
  - **Men√ß√µes** (ex: `@username`);
  - **Hashtags** (ex: `#emerg√™ncia`);
  - **Pontua√ß√£o e caracteres especiais** (ex: `!`, `?`, `...`);
  - **Emojis** e **s√≠mbolos n√£o textuais**;
  - **M√∫ltiplos espa√ßos em branco**.

---

**Nova coluna criada:** `text_clean`  
Essa coluna representa o conte√∫do textual limpo de cada tweet.

---

*Trecho de c√≥digo:*
```python
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import re

# Fun√ß√£o Python para limpeza
def limpar_texto(texto):
    if texto:
        texto = texto.lower()
        texto = re.sub(r"http\S+", "", texto)                     # Remove links
        texto = re.sub(r"rt\s+@[\w_]+:? ?", "", texto)            # Remove RTs com @usuario
        texto = re.sub(r"@\w+", "", texto)                        # Remove todas as outras men√ß√µes
        texto = re.sub(r"#\w+", "", texto)                        # Remove hashtags
        texto = re.sub(r"[^\w\s]", "", texto)                     # Remove pontua√ß√£o
        texto = re.sub(r"\s+", " ", texto).strip()                # Espa√ßos extras
    return texto

# Registrando a fun√ß√£o como UDF do PySpark
limpar_udf = udf(limpar_texto, StringType())

# Criando nova coluna com texto limpo
df_limpo = df_total.withColumn("text_clean", limpar_udf(col("tweet_text")))

# Visualizando resultado
df_limpo.select("tweet_text", "text_clean").show(10, truncate=False)
```

---

### 1.3 Tokeniza√ß√£o e remo√ß√£o de stopwords

**Objetivo:**  
Transformar os textos limpos em **listas de palavras relevantes**, removendo palavras comuns (como "the", "and", "is", etc.) que n√£o agregam valor sem√¢ntico. Essa etapa √© essencial para preparar os dados para an√°lise textual e modelagem supervisionada.

---

**Transforma√ß√µes aplicadas:**

- ‚úÖ **Tokeniza√ß√£o:** separa√ß√£o do texto em palavras individuais (*tokens*);
- ‚úÖ **Remo√ß√£o de stopwords:** exclus√£o de palavras muito frequentes e com pouco significado no contexto de NLP;
- ‚úÖ **Nova coluna gerada:** `tokens_filtrados`, contendo as listas de palavras √∫teis.

---

*Trecho de c√≥digo:*
```python
# Etapa 1: Tokeniza√ß√£o
from pyspark.ml.feature import Tokenizer

tokenizer = Tokenizer(inputCol="text_clean", outputCol="tokens")
df_tokens = tokenizer.transform(df_limpo)

# Etapa 2: Remo√ß√£o de stopwords
from pyspark.ml.feature import StopWordsRemover

remover = StopWordsRemover(inputCol="tokens", outputCol="tokens_filtrados")
df_tokens_filtrados = remover.transform(df_tokens)

# Visualizando o resultado
display(df_tokens_filtrados.head(5))
```

---

### 1.4 Salvamento em formato Parquet

**Objetivo:**  
Persistir os dados pr√©-processados em um formato otimizado (`Parquet`), garantindo **efici√™ncia de leitura**, **compacta√ß√£o de armazenamento** e **compatibilidade com processamento distribu√≠do** em etapas futuras do projeto.

---

**Por que Parquet?**

- ‚úÖ Formato **colunar**, ideal para consultas anal√≠ticas;
- ‚úÖ Compat√≠vel com ferramentas distribu√≠das como PySpark;
- ‚úÖ Permite leitura seletiva de colunas, economizando tempo e recursos;
- ‚úÖ Otimiza a performance de etapas como vetoriza√ß√£o e modelagem.

---

*Trecho de c√≥digo:*
```python
# Salvar DataFrame como .parquet
df_tokens_filtrados.write.mode("overwrite").parquet("/tmp/humaid_dados_limpos")

# Leitura futura:
df_pronto= spark.read.parquet("/tmp/humaid_dados_limpos")
df_pronto.show(5, truncate=False)
```

---

## üîπ M√≥dulo 2 ‚Äì An√°lise Explorat√≥ria de Dados (EDA)

**Objetivo:**  
Compreender a estrutura, distribui√ß√£o e principais caracter√≠sticas dos tweets humanit√°rios ap√≥s o pr√©-processamento. Essa etapa permite identificar padr√µes, anomalias e orientar decis√µes futuras na modelagem.

---

### 2.1 Vis√£o Geral dos Dados

**Objetivo da etapa:**  
Realizar uma an√°lise inicial para entender:
- O volume total de registros dispon√≠veis;
- A estrutura e os tipos das colunas;
- Presen√ßa de valores nulos;
- Exemplo de registros ap√≥s limpeza.

---

*Trecho de c√≥digo:*
```python
# Contar o total de linhas
print("Total de tweets:", df_tokens_filtrados.count())

# Ver categorias √∫nicas
df_tokens_filtrados.select("class_label").distinct().show(truncate=False)

# Verificando a contagem de linhas por categoria
df_tokens_filtrados.groupby("class_label").count().orderBy("count", ascending=False).show(truncate=False)
```

---

### 2.2 Distribui√ß√£o das Categorias (Balanceamento)

**Objetivo:**  
Analisar a vari√°vel-alvo `class_label` para entender o **balanceamento entre as categorias humanit√°rias**. Isso √© crucial para identificar poss√≠veis desequil√≠brios que possam influenciar negativamente os modelos de classifica√ß√£o supervisionada.

---

**Transforma√ß√µes e a√ß√µes aplicadas:**

- ‚úÖ Agrupamento por categoria com contagem de ocorr√™ncias;
- ‚úÖ Ordena√ß√£o decrescente para facilitar a an√°lise;
- ‚úÖ Gera√ß√£o de gr√°fico de barras para visualiza√ß√£o do desequil√≠brio.

---

*Trecho de c√≥digo:*
```python
# Agrupar em categorias
df_labels = df_tokens_filtrados.groupBy("class_label").count()

# Ordenar categorias do maior para o menor
df_labels = df_labels.orderBy("count",ascending=False)

# Converter para o pandas
df_labels = df_labels.toPandas()
```

---

### 2.3 Frequ√™ncia de Palavras por Categoria (An√°lise Quantitativa)

**Objetivo:**  
Identificar as **palavras mais frequentes dentro de cada categoria humanit√°ria**, a fim de entender os termos mais caracter√≠sticos de cada tipo de situa√ß√£o. Essa an√°lise ajuda na constru√ß√£o de vetores de texto mais representativos para os modelos.

---

**Transforma√ß√µes e a√ß√µes aplicadas:**

- ‚úÖ Explos√£o da coluna `tokens_filtrados` em palavras individuais;
- ‚úÖ Agrupamento por `class_label` + `token`;
- ‚úÖ Contagem de ocorr√™ncias de cada palavra por categoria;
- ‚úÖ Filtragem das palavras mais frequentes por classe.

---

*Trecho de c√≥digo:*
```python
# Selecionar categorias dominantes
categorias_dominantes = ["rescue_volunteering_or_donation_effort","other_relevant_information","infrastructure_and_utility_damage","sympathy_and_support","injured_or_dead  _people"]

# Filtrar nosso DataFrame com as categorias dominantes
df_filtrado = df_tokens_filtrados.filter(df_tokens_filtrados["class_label"].isin(categorias_dominantes))

df_filtrado.show(5)

# Gerar texto concatenado com categoria
from pyspark.sql.functions import explode, collect_list, concat_ws

# Vamos explodir as palavras
df_explode = df_filtrado.select("class_label", explode("tokens_filtrados").alias("token"))

df_explode.show(5)
```

---

### 2.4 Frequ√™ncia de N-Gramas (Bigramas e Trigramas)

**Objetivo:**  
Explorar **composi√ß√µes de palavras mais comuns** (bigramas e trigramas) nos tweets humanit√°rios. Isso permite capturar estruturas lingu√≠sticas relevantes, como ‚Äúneed food‚Äù, ‚Äúmedical help‚Äù, ou ‚Äúpeople are trapped‚Äù, que carregam mais contexto do que palavras isoladas.

---

**Transforma√ß√µes e a√ß√µes aplicadas:**

- ‚úÖ Utiliza√ß√£o do `NGram` do PySpark para gerar bigramas e trigramas a partir da coluna `tokens_filtrados`;
- ‚úÖ Explos√£o e contagem de n-gramas;
- ‚úÖ An√°lise das express√µes mais frequentes por categoria.

---

*Trecho de c√≥digo (Bigramas):*
```python
from pyspark.sql.functions import size

df_tokens_filtrados = df_tokens_filtrados.withColumn("text_length", size(col("tokens_filtrados")))
df_tokens_filtrados.show(5)

# Estat√≠sticas das palavras
df_tokens_filtrados.select("text_length").describe().show()
```

---

### 2.5 Comprimento dos Textos

**Objetivo:**  
Analisar a **distribui√ß√£o do comprimento dos textos**, medido em n√∫mero de tokens, para entender o n√≠vel de complexidade lingu√≠stica e poss√≠veis padr√µes associados a diferentes categorias humanit√°rias.

---

**Transforma√ß√µes e a√ß√µes aplicadas:**

- ‚úÖ C√°lculo da quantidade de tokens em cada tweet (coluna `tokens_filtrados`);
- ‚úÖ Gera√ß√£o de histogramas para visualizar a distribui√ß√£o geral e por categoria;
- ‚úÖ Identifica√ß√£o de outliers e padr√µes relevantes (ex: categorias com textos mais curtos ou longos).

---

*Trecho de c√≥digo:*
```python
# Explodir palavras por linha
from pyspark.sql.functions import explode

df_exploded_limpo = df_tokens_filtrados_limpos.select("class_label", explode("tokens_filtrados").alias("token"))

# Agrupa por categoria
from pyspark.sql.functions import collect_list

df_grouped_tokens = df_exploded_limpo.groupBy("class_label") \
    .agg(collect_list("token").alias("all_tokens"))

# Gerar unigramas, bigramas e trigramas

import pandas as pd
from nltk.util import ngrams
from collections import Counter

df_tokens_pd = df_grouped_tokens.toPandas()

def extrair_top_ngrams(tokens, n, top_n=10):
    return Counter(ngrams(tokens, n)).most_common(top_n)

rows = []

for _, row in df_tokens_pd.iterrows():
    categoria = row["class_label"]
    tokens = row["all_tokens"]

    # Top palavras, bigramas e trigramas
    top_unigrams = [uni[0] for uni in Counter(tokens).most_common(10)]
    top_bigrams = [" ".join(bi) for bi, _ in extrair_top_ngrams(tokens, 2)]
    top_trigrams = [" ".join(tri) for tri, _ in extrair_top_ngrams(tokens, 3)]

    rows.append({
        "class_label": categoria,
        "top_10_unigrams": ", ".join(top_unigrams),
        "top_10_bigrams": ", ".join(top_bigrams),
        "top_10_trigrams": ", ".join(top_trigrams),
    })

df_ngrams_limpo = pd.DataFrame(rows)
```
---

### 2.6 Correla√ß√µes entre R√≥tulos e Padr√µes Lingu√≠sticos

**Objetivo:**  
Investigar se existem **correla√ß√µes entre caracter√≠sticas lingu√≠sticas** (como comprimento do texto e presen√ßa de determinadas palavras) e os r√≥tulos das categorias humanit√°rias. Essa an√°lise ajuda a entender **quais padr√µes o modelo pode aprender para distinguir entre as classes**.

---

**Transforma√ß√µes e a√ß√µes aplicadas:**

- ‚úÖ Uso da coluna `text_length` criada anteriormente;
- ‚úÖ An√°lise de palavras-chave mais frequentes por categoria;
- ‚úÖ C√°lculo de medidas estat√≠sticas b√°sicas para comparar os grupos.

---

*Trecho de c√≥digo ‚Äì Distribui√ß√£o do comprimento por categoria:*
```python
# Frequ√™ncia de palavras por categoria

from pyspark.sql.functions import count

df_freq = df_exploded_limpo.groupBy("class_label", "token").agg(count("*").alias("freq"))
df_freq.show(5, truncate=False)

# Converter para pandas
df_freq_pd = df_freq.toPandas()

# Obter top 10 palavras por categoria
top_tokens_por_categoria = (
    df_freq_pd.sort_values("freq", ascending=False)
    .groupby("class_label")
    .head(10)
    .reset_index(drop=True)
)
```

---

## üîπ M√≥dulo 3 ‚Äì Engenharia de Features e Modelagem

**Objetivo:**  
Transformar os textos em representa√ß√µes num√©ricas (vetores) que possam ser compreendidas por algoritmos de Machine Learning. Em seguida, treinar modelos supervisionados para prever a categoria humanit√°ria de novos tweets com base em seus conte√∫dos.

---

### 3.1 Vetoriza√ß√£o com TF-IDF (Scikit-learn)

**Objetivo:**  
Transformar os textos dos tweets em vetores num√©ricos utilizando a t√©cnica **TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)** com o `TfidfVectorizer` do Scikit-learn. Essa t√©cnica permite que os modelos identifiquem quais palavras (ou sequ√™ncias de palavras) s√£o mais relevantes para classificar corretamente cada tipo de emerg√™ncia.

---

**Transforma√ß√µes e a√ß√µes aplicadas:**

- ‚úÖ Reconstru√ß√£o do texto original a partir dos tokens filtrados;
- ‚úÖ Convers√£o do DataFrame PySpark para Pandas;
- ‚úÖ Aplica√ß√£o do `TfidfVectorizer` com n-gramas (1 a 3 palavras);
- ‚úÖ Cria√ß√£o da matriz esparsa `X` com at√© 5.000 features.

---

*Trecho de c√≥digo:*
```python
# Juntar palavras no texto
from pyspark.sql.functions import concat_ws

df_texto = df_tokens_filtrados_limpos.select(
    "class_label",
    concat_ws(" ", "tokens_filtrados").alias("texto")
)

# Converter para Pandas
df_modelo = df_texto.toPandas()

# Vetoriza√ß√£o com TF-IDF + N-gramas

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=5000)
X = vectorizer.fit_transform(df_modelo["texto"])
```

---

### 3.2 Treinamento de modelos supervisionados (LogReg, Random Forest)

**Objetivo:**  
Treinar modelos de classifica√ß√£o supervisionada utilizando os vetores TF-IDF gerados anteriormente, com o objetivo de prever a categoria (`class_label`) de cada tweet humanit√°rio.

---

**Modelos utilizados:**

- ‚úÖ **Logistic Regression (LogReg):** modelo linear bastante eficiente para classifica√ß√µes multiclasses com vetores esparsos.
- ‚úÖ **Random Forest Classifier:** modelo de ensemble baseado em √°rvores de decis√£o, mais robusto a n√£o-linearidades e varia√ß√µes.

---

*Trecho de c√≥digo:*
```python
# Separar treino e teste
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Treinar o modelo Regress√£o Logistica
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(max_iter=1000, random_state=42)
logreg.fit(X_train, y_train)
```

---

### 3.3 Avalia√ß√£o com F1-Score e Matriz de Confus√£o

**Objetivo:**  
Avaliar o desempenho dos modelos Logistic Regression e Random Forest utilizando m√©tricas apropriadas para classifica√ß√£o multiclasse, com foco em **F1-Score**, **Acur√°cia** e **Matriz de Confus√£o**.

---

**M√©tricas utilizadas:**

- ‚úÖ **F1-Score (macro):** m√©dia harm√¥nica entre precis√£o e recall, ponderada igualmente entre as classes;
- ‚úÖ **Acur√°cia:** propor√ß√£o de classifica√ß√µes corretas sobre o total;
- ‚úÖ **Matriz de Confus√£o:** mostra visualmente os acertos e erros por classe.

---

**A√ß√µes aplicadas:**

- Gera√ß√£o de predi√ß√µes no conjunto de teste;
- Visualiza√ß√£o da matriz de confus√£o com Pandas e Seaborn.

---

*Trecho de c√≥digo:*
```python
# Avaliar os dois modelos
from sklearn.metrics import classification_report, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Avalia√ß√£o - Logistic Regression
y_pred_logreg = logreg.predict(X_test)
print("üìä Logistic Regression:")
print(classification_report(y_test, y_pred_logreg))

# Avalia√ß√£o - Random Forest
y_pred_rf = rf.predict(X_test)
print("üìä Random Forest:")
print(classification_report(y_test, y_pred_rf))
```

---

### 3.4 Tracking de experimentos com MLflow

**Objetivo:**  
Rastrear os experimentos de machine learning com o MLflow, registrando automaticamente m√©tricas, par√¢metros, modelos e artefatos. Isso permite **comparar modelos, reproduzir resultados e documentar a evolu√ß√£o do projeto de forma profissional**.

---

**Recursos monitorados:**

- ‚úÖ M√©tricas: F1-score, acur√°cia;
- ‚úÖ Par√¢metros dos modelos (ex: `maxIter`, `numTrees`);
- ‚úÖ Artefatos: modelo treinado, matriz de confus√£o, configura√ß√µes;
- ‚úÖ Tags e anota√ß√µes manuais para melhor organiza√ß√£o.

---

**Configura√ß√£o inicial:**
```python
import mlflow
import mlflow.sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score

# Caminho v√°lido dentro do Databricks
experiment_name = "/Shared/llm-smart-classifier"

# Cria o experimento se n√£o existir
experiment = mlflow.get_experiment_by_name(experiment_name)
if experiment is None:
    experiment_id = mlflow.create_experiment(experiment_name)
else:
    experiment_id = experiment.experiment_id

# Inicia o tracking com o experimento criado
with mlflow.start_run(experiment_id=experiment_id):
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mlflow.log_param("model", "LogisticRegression")
    mlflow.log_param("max_iter", 1000)
    mlflow.log_param("ngram_range", (1, 3))
    mlflow.log_param("max_features", 5000)

    mlflow.log_metric("accuracy", accuracy_score(y_test, y_pred))
    mlflow.log_metric("f1_weighted", f1_score(y_test, y_pred, average="weighted"))

    mlflow.sklearn.log_model(model, "model")
```

---



