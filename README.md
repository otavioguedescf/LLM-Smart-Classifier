# üß† LLM Smart Classifier

Projeto completo de classifica√ß√£o inteligente de tweets humanit√°rios, utilizando Big Data com PySpark, Machine Learning supervisionado, Modelos de Linguagem (LLMs), MLOps e deploy em nuvem.

---

## üöÄ Vis√£o Geral

Este projeto tem como objetivo construir uma solu√ß√£o capaz de **entender, categorizar e explicar automaticamente milhares de mensagens humanit√°rias** postadas em redes sociais durante crises e emerg√™ncias.

Por meio de uma abordagem moderna e escal√°vel, unimos t√©cnicas de NLP com modelos cl√°ssicos de machine learning, modelos de linguagem (LLMs), pipelines de MLOps e interfaces de uso para entregar um sistema robusto e aplic√°vel em cen√°rios reais.

---

## üß© Estrutura Modular do Projeto

| M√≥dulo | Descri√ß√£o |
|--------|-----------|
| [1. Ingest√£o e Pr√©-processamento](#modulo-1--ingestao-e-pre-processamento-com-pyspark) | Leitura distribu√≠da dos dados com PySpark, limpeza e tokeniza√ß√£o |
| [2. An√°lise Explorat√≥ria de Dados](#modulo-2--analise-exploratoria-de-dados-eda) | Explora√ß√£o, frequ√™ncia de palavras, n-gramas e padr√µes |
| [3. Engenharia de Features e Modelagem](#modulo-3--engenharia-de-features-e-modelagem) | Vetoriza√ß√£o, modelos supervisionados e tracking com MLFlow |
| [4. Aplica√ß√£o de LLMs](#modulo-4--aplicacao-de-llms) | Embeddings, zero-shot, few-shot, explica√ß√µes e compara√ß√£o |
| [5. MLOps e Deploy](#modulo-5--mlops-e-deploy) | Pipeline, API, Docker, Azure ML, CI/CD |
| [6. Valida√ß√£o Avan√ßada](#modulo-6--validacao-avancada-e-ab-testing) | Testes estat√≠sticos e compara√ß√£o entre abordagens |
| [7. Interface Visual Interativa](#modulo-7--interface-visual-interativa) | App com input e explica√ß√µes acess√≠veis |
| [8. Documenta√ß√£o e Storytelling T√©cnico](#modulo-8--documentacao-e-storytelling-tecnico) | README, prints, impacto e vis√£o estrat√©gica |

---

## üîπ M√≥dulo 1 ‚Äì Ingest√£o e Pr√©-processamento com PySpark

**Objetivo:**  
Ingerir os dados brutos no ambiente distribu√≠do do PySpark, realizar o pr√©-processamento textual necess√°rio para an√°lise posterior e salvar os dados limpos em formato otimizado (Parquet).

---

### 1.1 Leitura do dataset no ambiente distribu√≠do

- O dataset foi carregado diretamente no ambiente Databricks com PySpark, contendo aproximadamente **47.868 tweets** humanit√°rios.
- Cada linha representa uma mensagem classificada por categoria humanit√°ria (como sa√∫de, abrigo, comida, etc.).

*Trecho de c√≥digo:*
```python
# Copiando o ZIP da DBFS para o sistema de arquivos local do cluster
dbutils.fs.cp("dbfs:/FileStore/tables/humaid_eventwise.zip", "file:/tmp/humaid_eventwise.zip")

# Extraindo
with zipfile.ZipFile("/tmp/humaid_eventwise.zip", 'r') as zip_ref:
    zip_ref.extractall("/tmp/humaid_eventwise")
```
---

### 1.2 Limpeza textual e normaliza√ß√£o

**Objetivo:**  
Preparar os textos dos tweets para processamento lingu√≠stico, removendo ru√≠dos e padronizando a estrutura textual. Esta etapa √© essencial para melhorar a qualidade dos dados e garantir resultados mais precisos nos pr√≥ximos m√≥dulos de an√°lise e modelagem.

---

**Transforma√ß√µes aplicadas:**

- ‚úÖ Convers√£o de todos os caracteres para min√∫sculas;
- ‚úÖ Remo√ß√£o de:
  - **Links/URLs** (ex: `http://...`);
  - **Men√ß√µes** (ex: `@username`);
  - **Hashtags** (ex: `#emerg√™ncia`);
  - **Pontua√ß√£o e caracteres especiais** (ex: `!`, `?`, `...`);
  - **Emojis** e **s√≠mbolos n√£o textuais**;
  - **M√∫ltiplos espa√ßos em branco**.

---

**Nova coluna criada:** `text_clean`  
Essa coluna representa o conte√∫do textual limpo de cada tweet.

---

*Trecho de c√≥digo:*
```python
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType
import re

# Fun√ß√£o Python para limpeza
def limpar_texto(texto):
    if texto:
        texto = texto.lower()
        texto = re.sub(r"http\S+", "", texto)                     # Remove links
        texto = re.sub(r"rt\s+@[\w_]+:? ?", "", texto)            # Remove RTs com @usuario
        texto = re.sub(r"@\w+", "", texto)                        # Remove todas as outras men√ß√µes
        texto = re.sub(r"#\w+", "", texto)                        # Remove hashtags
        texto = re.sub(r"[^\w\s]", "", texto)                     # Remove pontua√ß√£o
        texto = re.sub(r"\s+", " ", texto).strip()                # Espa√ßos extras
    return texto

# Registrando a fun√ß√£o como UDF do PySpark
limpar_udf = udf(limpar_texto, StringType())

# Criando nova coluna com texto limpo
df_limpo = df_total.withColumn("text_clean", limpar_udf(col("tweet_text")))

# Visualizando resultado
df_limpo.select("tweet_text", "text_clean").show(10, truncate=False)
```
---

### 1.3 Tokeniza√ß√£o e remo√ß√£o de stopwords

**Objetivo:**  
Transformar os textos limpos em **listas de palavras relevantes**, removendo palavras comuns (como "the", "and", "is", etc.) que n√£o agregam valor sem√¢ntico. Essa etapa √© essencial para preparar os dados para an√°lise textual e modelagem supervisionada.

---

**Transforma√ß√µes aplicadas:**

- ‚úÖ **Tokeniza√ß√£o:** separa√ß√£o do texto em palavras individuais (*tokens*);
- ‚úÖ **Remo√ß√£o de stopwords:** exclus√£o de palavras muito frequentes e com pouco significado no contexto de NLP;
- ‚úÖ **Nova coluna gerada:** `tokens_filtrados`, contendo as listas de palavras √∫teis.

---

*Trecho de c√≥digo:*
```python
# Etapa 1: Tokeniza√ß√£o
from pyspark.ml.feature import Tokenizer

tokenizer = Tokenizer(inputCol="text_clean", outputCol="tokens")
df_tokens = tokenizer.transform(df_limpo)

# Etapa 2: Remo√ß√£o de stopwords
from pyspark.ml.feature import StopWordsRemover

remover = StopWordsRemover(inputCol="tokens", outputCol="tokens_filtrados")
df_tokens_filtrados = remover.transform(df_tokens)

# Visualizando o resultado
display(df_tokens_filtrados.head(5))
```
---

### 1.4 Salvamento em formato Parquet

**Objetivo:**  
Persistir os dados pr√©-processados em um formato otimizado (`Parquet`), garantindo **efici√™ncia de leitura**, **compacta√ß√£o de armazenamento** e **compatibilidade com processamento distribu√≠do** em etapas futuras do projeto.

---

**Por que Parquet?**

- ‚úÖ Formato **colunar**, ideal para consultas anal√≠ticas;
- ‚úÖ Compat√≠vel com ferramentas distribu√≠das como PySpark;
- ‚úÖ Permite leitura seletiva de colunas, economizando tempo e recursos;
- ‚úÖ Otimiza a performance de etapas como vetoriza√ß√£o e modelagem.

---

*Trecho de c√≥digo:*
```python
# Salvar DataFrame como .parquet
df_tokens_filtrados.write.mode("overwrite").parquet("/tmp/humaid_dados_limpos")

# Leitura futura:
df_pronto= spark.read.parquet("/tmp/humaid_dados_limpos")
df_pronto.show(5, truncate=False)
```
---

## üîπ M√≥dulo 2 ‚Äì An√°lise Explorat√≥ria de Dados (EDA)

**Objetivo:**  
Compreender a estrutura, distribui√ß√£o e principais caracter√≠sticas dos tweets humanit√°rios ap√≥s o pr√©-processamento. Essa etapa permite identificar padr√µes, anomalias e orientar decis√µes futuras na modelagem.

---

### 2.1 Vis√£o Geral dos Dados

**Objetivo da etapa:**  
Realizar uma an√°lise inicial para entender:
- O volume total de registros dispon√≠veis;
- A estrutura e os tipos das colunas;
- Presen√ßa de valores nulos;
- Exemplo de registros ap√≥s limpeza.

---

*Trecho de c√≥digo:*
```python
# Contar o total de linhas
print("Total de tweets:", df_tokens_filtrados.count())

# Ver categorias √∫nicas
df_tokens_filtrados.select("class_label").distinct().show(truncate=False)

# Verificando a contagem de linhas por categoria
df_tokens_filtrados.groupby("class_label").count().orderBy("count", ascending=False).show(truncate=False)
```

---

### 2.2 Distribui√ß√£o das Categorias (Balanceamento)

**Objetivo:**  
Analisar a vari√°vel-alvo `class_label` para entender o **balanceamento entre as categorias humanit√°rias**. Isso √© crucial para identificar poss√≠veis desequil√≠brios que possam influenciar negativamente os modelos de classifica√ß√£o supervisionada.

---

**Transforma√ß√µes e a√ß√µes aplicadas:**

- ‚úÖ Agrupamento por categoria com contagem de ocorr√™ncias;
- ‚úÖ Ordena√ß√£o decrescente para facilitar a an√°lise;
- ‚úÖ Gera√ß√£o de gr√°fico de barras para visualiza√ß√£o do desequil√≠brio.

---

*Trecho de c√≥digo:*
```python
# Agrupar em categorias
df_labels = df_tokens_filtrados.groupBy("class_label").count()

# Ordenar categorias do maior para o menor
df_labels = df_labels.orderBy("count",ascending=False)

# Converter para o pandas
df_labels = df_labels.toPandas()
```
---
